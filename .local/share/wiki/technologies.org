

[[file:index.org][index]]

* Statistics
** Correlation
pueden ser negativos tambien, los rangos aplican igual 
R=1 o R=-1      correlacion Perfecta
0.9 <= R < 1    correlacion Excelente
0.8 <= R < 0.9  correlacion aceptable
0.6 <= R < 0.8  correlacion regular
0.3 <= R < 0.6  correlacion minima
0.0 <= R < 0.3  correlacion no hay
** Hipothesis test
1-alptha = confidence
alpha = 0.05 investigations
alpha = 0.01 prove quality
alpha = 0.10 stuff that has to do with humans(volatility)

populations < 30 you do a t-student test
else a Z (normality) test
*** Scores

they are used to prove if an hypothesis is right or wrong, using the alphas => 1-alpha
https://www.simplypsychology.org/z-score.html
https://www.statisticshowto.com/standardized-test-statistic/
Z-scores for normal samples
T-scores for normal samples population is less than 30
*** Parametric tests
y   ou use the scores (using the formulas)
*** non parametric tests
**** prueba del signo
    se hacen diferencias entre los datos, solo interesa el signo de la diferencia, se cuentan los positivos y negativos y se saca el z-score 
    
**** correlacion por rangos  (menor a 25 usar t student)

    se ordenan de menor a mayor, se sacan rangos usando la posicion ordenada (inicial - final)/2 de un numero repetido, si no esta repetido, entonces
    su rango es su posicion. Se sacan diferencias de los rangos de los 2 grupos y se usa la formula al final, a este resultado se le llama Rs y se sustituye
    en la formula para hallar t-student o z-score (ajustada para esta variable Rs)
    
**** Wilcoxon (menor 24 usar t-student)
  
    se usa para parejas de datos como (esposa esposo)
    
**** Man whitney (t-student si menor a 20)
las muestras no tienen que ser de mismo tamaño!
se sacan rangos igual que correlacion por rangos, luego se suman los rangos de cada muestra , quedando con los 2 valores del rango para ambas muestras

finalmente se escoje el rango mas pequeño de los 2 y se substituye en la formula para encontrar un U, luego se encuentra el z-score o t-score (modificados para usar U)
y se prueba la hipotesis

**** Kruskal watts (muestras independientes, puedens ser mas de 2)
  se juntan todos los datos de todas las muestras, se ordenan, se encuentran los rangos igual que en el correlacion por rangos
  luego de encontrar los rangos, se separan los datos como estaban y se suman los rangos totales para cada muestra, quedamos con que cada muestra tiene un rango total
  encontramos H con la formula que usa los rangos, este es el H-score, se calcula la grafica usando xi cuadrado , el error es: Xi(alpha, k-1) donde k es el numero de muestras 
el resultado de Xi dara el punto donde comienza el error

**** Xi cuadrado
  para verificar si los datos son consistentes con el modelo
  chi-square test is used to determine whether there is a statistically significant difference between the expected frequencies and the observed
  frequencies in one or more categories of a contingency table. 
  
**** Correccion de yates
 is used in certain situations when testing for independence in a contingency table.
 It aims at correcting the error introduced by assuming that the discrete probabilities of frequencies in the table
 can be approximated by a continuous distribution (chi-squared).
 In some cases, Yates's correction may adjust too far, and so its current use is limited. 
 
se utiliza Xi cuadrado para encontrar donde inicia el error, se usan las formulas para encontrar el Xi-score
 
***** a contingency table
In statistics, a contingency table (also known as a cross tabulation or crosstab) is a type of table in a matrix format that
 displays the (multivariate) frequency distribution of the variables. They are heavily used in survey research,
 business intelligence, engineering and scientific research.
   
** Anova
se debe de verificar 
    1. Independencia de las variables
    2. Shapiro-wilk test (normality)
    3. bartlet test (homocedasticity)
    4. (correlacion )? 
       
si no cumple, usar tukey o duncan para encontrar cual es la media diferente

* Machine Learning

book hands on machine learning
there are 2 aproaches of generalization, instance based and model based
** Instance based
can make a square be a triangle because the closest ones in the training were triangles,
it learns the training instances by heart
*** example K nearest neighbors

is the same as the linear model, but with this change 

model = sklearn.linear_model.()

becomes 

model = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)
 
page 22 

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import sklearn.linear_model

# Load the data
oecd_bli = pd.read_csv("oecd_bli_2015.csv", thousands=',')
gdp_per_capita = pd.read_csv("gdp_per_capita.csv",thousands=',',delimiter='\t',
encoding='latin1', na_values="n/a")

# Prepare the data
country_stats = prepare_country_stats(oecd_bli, gdp_per_capita)
X = np.c_[country_stats["GDP per capita"]]
y = np.c_[country_stats["Life satisfaction"]]

# Visualize the data
country_stats.plot(kind='scatter', x="GDP per capita", y='Life satisfaction')
plt.show()
# Select a linear model
model = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)
# Train the model
model.fit(X, y)
# Make a prediction for Cyprus
X_new = [[22587]] # Cyprus' GDP per capita
print(model.predict(X_new)) # outputs [[ 5.96242338]]
6 The prepare_country_stats() function’s definition is not shown here (see this chapter’s Jupyter notebook

** Model based
linear model (linear regression) : W + B * X
*** example linear regression

page 22 

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import sklearn.linear_model

# Load the data
oecd_bli = pd.read_csv("oecd_bli_2015.csv", thousands=',')
gdp_per_capita = pd.read_csv("gdp_per_capita.csv",thousands=',',delimiter='\t',
encoding='latin1', na_values="n/a")

# Prepare the data
country_stats = prepare_country_stats(oecd_bli, gdp_per_capita)
X = np.c_[country_stats["GDP per capita"]]
y = np.c_[country_stats["Life satisfaction"]]

# Visualize the data
country_stats.plot(kind='scatter', x="GDP per capita", y='Life satisfaction')
plt.show()
# Select a linear model
model = sklearn.linear_model.LinearRegression()
# Train the model
model.fit(X, y)
# Make a prediction for Cyprus
X_new = [[22587]] # Cyprus' GDP per capita
print(model.predict(X_new)) # outputs [[ 5.96242338]]
6 The prepare_country_stats() function’s definition is not shown here (see this chapter’s Jupyter notebook
** instances
also called samples
there are training instances which are simply data instances of that training
see page 12-13 
** Types of machine learning
*** Supervised

training: when you have data and you know the output (labels)

some of the most important are

**** k-Nearest Neighbors
**** Linear Regression
**** Logistic Regression   //predict 1 of 2 values (like buy or not 2 buy) 
**** Support Vector Machines (SVMs)
**** Decision Trees and Random Forests
**** Neural networks

*** Unsupervised
training: when you have data but no output (group data in clusters)



**** Clustering Algorithms
***** K-means
***** DBSCAN
***** Hierarchical Cluster Analysis (HCA)
**** Anomaly detection and novelty detection
***** One class SVM
***** Isolation forest
**** others
• Visualization and dimensionality reduction
    — Principal Component Analysis (PCA)
    — Kernel PCA
    — Locally-Linear Embedding (LLE)
    — t-distributed Stochastic Neighbor Embedding (t-SNE)
• Association rule learning
    — Apriori
    — Ecla
*** Semisupervised
deep belief networks (DBNs)
*** Reinforcement
the learning system called an agent observes the environment, performs actions and gets rewards or penalties in return
when you give positive points to something that gets you closer to your goal/flag
** Fundamental Algorithms
*** Linear Regression
The requirement to do a model is ANOVA
**** Analysis


***** 1 Normalidad 
    se analiza si los valores de y siguen una distribucion normal en cada valor de x
    It is also important to check for outliers since linear regression is sensitive to outlier effects
    +  When the data is not normally distributed, a non-linear transformation (e.g., log-transformation) might fix this issue.

***** 2. Homocedasticidad
la variacion en torno a la recta de regresion es constante para todo x, sin importar el valor que tome, de todas formas la variacion 
debe ser la misma

***** 3. Independencia del error
establece que el error de estimacion es independiente para cada valor de x (cada error es diferente)



**** Correlation
is r and r2 (squared)
for example, R2 = 0.8234 means that the linear model explains 82.34% of the variance
between the 2 variables
+ Pearson
  measures correlation
+ Spearman
  measures correlation using ranks, specially for ordinal variables

the correlation is done by drawind a line of best fit

***** line of best fit
can be calculated using the least square method: https://www.youtube.com/watch?v=JvS2triCgOY

line that gets as close as possible to most points
the line of best fit is used to predict future values
*** Classification
*** Clustering
*** Hidden Markov Models

* PyQt
** Qdialog and QWidget
they are the same, 

Qdialog shows messages or ask for input
    also, it has no X or minimiza button

Qwidget is more global
** layouts
    http://zetcode.com/gui/pyqt5/layout/
* Tensorflow
** types of problems
*** regression
    https://www.tensorflow.org/tutorials/keras/regression
we aim to predict the output of a continuous value, like a price or a probability.
*** classification
 classification problem, we aim to select a class from a list of classes (for example, where a picture contains an apple or an orange,
 recognizing which fruit is in the picture).
** Official examples
*** linear 
https://www.tensorflow.org/tutorials/estimator/linear
** Main components
*** Graphs
an equation definition (what is gonna be done)
*** fitness or cost function
functions that calculate the current cost using a bias and weights (and data)
*** Optimizer
recieves a learning rate, it tries to minimize or maximize something (the c ost)
*** Sessions
Executes certain parts of the graph (computation)
this is where training happens
** Training
first understand the data.
you are going to feed data into the model
*** Encoding data
tensorflow can transform categorical data like(male, female) into encodings, look up the methods
https://www.youtube.com/watch?v=tPYj3fFJGjk&t=4600 minute 1:35:00
*** Epoch

start with little amount of epochs, then you keep on increasing, to avoid overfeeding 

an epoc is a stream of the entire dataset. the number of epochs we define is the amount of times our model will see the entire dataset,
 we use multiple epochs in hope that after seeing the same data multiple times the model will better determine how to estimate it.

Ex. if we have 10 ephocs, our model will see the same dataset 10 times. 
**** overfeeding
overfeeding happens when you use too many Epochs, the model becomes very good at classifying the data we fed it, but because
it memorized the data points, once we feed different data is gonna fail

** Tensor
generalization of vectors and matrixes in potentially higher dimensions

Each tensor represents a partially defined computation that will eventually produce a value, which means that
tensors can be executed and you can get results from them
*** Types of Tensors
all are immutable except variables
+ Variables
+ Constants
+ Placeholder
+ SparseTensor
*** Evaluating Tensors/Getting value of a tensor
to get the value of a Tensor you need to create a session, like this

with tf.Session() as sess: # creates a session using the default graph
    name_of_my_tensor.eval()  

*** Rank/Degree
the deepest level of a nested list

+ Rank 1
    rank1_tensor = tf.Variable(["Test"], tf.string) 
+ Rank 2
    rank2_tensor = tf.Variable([["test", "ok"], ["test", "yes"]], tf.string)
    
tf.rank(rank2_tensor)
<tf.Tensor: shape=(), dtype=int32, numpy=2>

the numpy=2 means is of rank 2
*** example
string = tf.Variable("this is a string", tf.string) 
number = tf.Variable(324, tf.int16)
floating = tf.Variable(3.567, tf.float64)
*** Shape
all tensors have the attribute .shape
represents the number of items that exist in each dimension

rank2_tensor = tf.Variable([["test", "ok", "hey"], ["test", "yes", "why"]], tf.string)
+ rank2_tensor.shape 

  Tensorshape([2, 3]) which means, 2 lists, and each list has 3 elements
*** Change Shape
is possible to change shape as long as the number of elements corresponds to the current shape and the desired one

tensor1 = tf.ones([1,2,3])  # tf.ones() creates a shape [1,2,3] tensor full of ones
tensor2 = tf.reshape(tensor1, [2,3,1])  # reshape existing data to shape [2,3,1]
tensor3 = tf.reshape(tensor2, [3, -1])  # -1 tells the tensor to calculate the size of the dimension in that place
                                        # this will reshape the tensor to [3,3]
                                                                             
# The numer of elements in the reshaped tensor MUST match the number in the original

** Functions
tf.ones([shape,shape,...])
tf.zeros([shape,shape,...])
* Angular
start here: https://angular.io/tutorial
curso vence sep 30!!: https://courses.edx.org/courses/course-v1:Microsoft+DEV314x+1T2019a/course/
ecommerce fast: https://www.youtube.com/watch?v=gLa2LxMdAPs
ecommerce: https://www.youtube.com/watch?v=9K15zC0gN2I
best ecommerce: https://medium.com/javascript-in-plain-english/how-to-create-an-app-using-angular-and-firebase-part-1-debb80f928ad
* CSS
curso: https://courses.edx.org/courses/course-v1:W3Cx+CSS.0x+3T2018/course/
* React 
ecommerce: https://www.youtube.com/watch?v=wPQ1-33teR4
npx create-react-app
** react elements
+ theres html  like syntax (jsx)
const element = (
   <h1 className="greetings">
      hello, world!
   </h1>
);
gets mapped to 
const element = React.createElement(
    'h1',
    {className: 'greeting'},
    'Hello, World'

** react Components

you always extend the component, always create a constructor and a render, and always export it,

class Menu extends Component {
  constructor(props) {
    super(props);

  }
  
  render() {
    return (
        //jsx
    );

  }
  export default Menu
** react router
yarn add react-router-dom

-- in app.js
import { BrowserRouter } from 'react-router-dom';
<BrowserRouter>
      <div className="App">
        <Main />
      </div>
    </BrowserRouter>
    
-- in mainComponent


    render() {
    const HomePage = () => {
      return (
        <Home
          dish={this.state.dishes.filter(dish => dish.featured)[0]}
          promotion={this.state.promotions.filter(promo => promo.featured)[0]}
          leader={this.state.leaders.filter(leader => leader.featured)[0]}
        />
      );
    };
    // can be { match , location , history }

    const DishWithId = ({ match }) => {
      return (
        <DishDetail
          dish={
            this.state.dishes.filter(dish => dish.id === parseInt(match.params.dishId, 10))[0]
          }
          comments={this.state.comments.filter(
            comment => comment.dishId === parseInt(match.params.dishId, 10)
          )}
        />
      );
    };

    return (
      <div>
        <Header />
        <Switch>
          <Route path="/home" component={HomePage} />
          <Route exact path="/menu" component={() => <Menu dishes={this.state.dishes}/>}/>
          <Route path="/menu/:dishId" component={DishWithId} />
          <Route exact path="/contactus" component={Contact} />
          <Redirect to="/home" />
        </Switch>
        <Footer />
      </div>
    );
  }

*** links
the link is in backticks, and it sends the parameter dish.id as a prop, and is recieved in mainComponent in <Route path="/menu/:dishId">


    <Card>
      <Link to={`/menu/${dish.id}`}>
        <CardImg width="100%" src={dish.image} alt={dish.name} />
        <CardImgOverlay>
          <CardTitle>{dish.name}</CardTitle>
        </CardImgOverlay>
      </Link>
    </Card>

* Falcon
    https://www.alibabacloud.com/blog/building-very-fast-app-backends-with-falcon-web-framework-on-pypy_594282

* Docker-Compose
    https://gist.github.com/emmettna/b78f54a6683b06a2a2da21db7580a8d6
    https://www.youtube.com/watch?v=dVEjSmKFUVI
* Javascript
* PHP
* Nodejs
** --dev-save
it creates/appends the package to devdependencies in package.json
* Rust
** docs
rustup doc
** compile
rustc filename
** create project
cargo new name_app --bin
** unit tests
cargo test

to create a test:

#[test]
fn add() {

    assert_eq!(2+2, 4);
}

** project docs
cargo doc --open

** iterators
have defined functions and work like this
(0..10).sum

** match (same as case)

the underscore _ means anything else

match variable {

    0=>0,
    1=>1,
    _=> final,

}
** loops
there are for and while,

there are also loop, which can be used in expressions like this:

    let mut result = 1;
    result = loop {
        result += 1;
        if result == 10 {
            break result * 2
        }
    };
    /// result = 20

* awk
make colon be treated as a space

    awk -F: '{print $1}' /etc/group | head -4

* pdfgrep
it has most of the same settings as grep, it uses regex
-- means end of options similar to grep -e
so pdfgrep -- -v
insensitive
pdfgrep -i
* sed
** Backreferences
is better to use sed -r because it groups with () instead of \( \) 
which means you can use normal parenthesis like \( \) 

capture the entire line with &

    seq 15 | sed 's/.*/& sucks/'


* Python    
** build exe file
https://www.youtube.com/watch?v=8iCmDTp7WCY
building and compressing
https://www.youtube.com/watch?v=UZX5kH72Yx4

pyinstaller -w  makes it so theres no terminal window
pyinstaller -w --onefile, compresses everything to just one exe file (including libraries)
    but it becomes very slow to open
    
name_program.py
name_program.spec this one is generated by pyinstaller, you can add dependencies in hiddenimports

pyinstaller name_program.spec runs it according to that file

my final version simply was: pyinstaller -w mywindow.py
* Bash
** Command substitution resets \n
http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_03_04.html
3.4.5. Command substitution

Command substitution allows the output of a command to replace the command itself. Command substitution occurs when a command is enclosed like this:

$(command)

or like this using backticks:

`command`

Bash performs the expansion by executing COMMAND and replacing the command substitution with the standard output of the command, with any trailing newlines deleted. Embedded newlines are not deleted, but they may be removed during word splitting.
